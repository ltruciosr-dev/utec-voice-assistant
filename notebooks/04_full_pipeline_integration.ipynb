{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Voice Assistant Pipeline Integration\n",
    "\n",
    "This notebook integrates all three services (ASR, LLM, TTS) and measures end-to-end performance.\n",
    "\n",
    "**Pipeline Flow:**\n",
    "```\n",
    "Audio Input → ASR → Text → LLM → Response → TTS → Audio Output\n",
    "```\n",
    "\n",
    "**Measurements:**\n",
    "- Individual service timings\n",
    "- Total pipeline latency\n",
    "- GPU memory usage\n",
    "- Real-time performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from transformers import (\n",
    "    AutoModelForSpeechSeq2Seq, \n",
    "    AutoProcessor, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from TTS.api import TTS\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import Audio, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"  VOICE ASSISTANT PIPELINE - FULL INTEGRATION TEST\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Available Memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return {\n",
    "            \"allocated_gb\": torch.cuda.memory_allocated() / 1024**3,\n",
    "            \"reserved_gb\": torch.cuda.memory_reserved() / 1024**3,\n",
    "            \"peak_gb\": torch.cuda.max_memory_allocated() / 1024**3\n",
    "        }\n",
    "    return {\"allocated_gb\": 0, \"reserved_gb\": 0, \"peak_gb\": 0}\n",
    "\n",
    "def print_stage_header(stage_name, stage_num, total_stages):\n",
    "    \"\"\"Print a formatted stage header.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"  [{stage_num}/{total_stages}] {stage_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "def record_audio(duration=5, sample_rate=16000):\n",
    "    \"\"\"Record audio from microphone.\"\"\"\n",
    "    print(f\"\\n🎤 Recording for {duration} seconds...\")\n",
    "    print(\"Speak now!\\n\")\n",
    "    \n",
    "    recording = sd.rec(\n",
    "        int(duration * sample_rate),\n",
    "        samplerate=sample_rate,\n",
    "        channels=1,\n",
    "        dtype='float32'\n",
    "    )\n",
    "    sd.wait()\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"pipeline_input_{timestamp}.wav\"\n",
    "    sf.write(filename, recording, sample_rate)\n",
    "    \n",
    "    print(f\"✓ Recording saved: {filename}\\n\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load ASR Model (Whisper Small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stage_header(\"Loading ASR Model\", 1, 3)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "print(\"Loading Whisper Small...\")\n",
    "start_time = time.time()\n",
    "\n",
    "asr_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    \"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")\n",
    "asr_model.to(device)\n",
    "\n",
    "asr_processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "asr_pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=asr_model,\n",
    "    tokenizer=asr_processor.tokenizer,\n",
    "    feature_extractor=asr_processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "asr_memory = get_gpu_memory()\n",
    "\n",
    "print(f\"\\n✓ ASR loaded in {load_time:.2f}s\")\n",
    "print(f\"GPU Memory: {asr_memory['allocated_gb']:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load LLM Model (Qwen2.5-7B-Instruct 4-bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stage_header(\"Loading LLM Model\", 2, 3)\n",
    "\n",
    "print(\"Loading Qwen2.5-7B-Instruct with 4-bit quantization...\")\n",
    "start_time = time.time()\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if llm_tokenizer.pad_token is None:\n",
    "    llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "llm_memory = get_gpu_memory()\n",
    "\n",
    "print(f\"\\n✓ LLM loaded in {load_time:.2f}s\")\n",
    "print(f\"GPU Memory: {llm_memory['allocated_gb']:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load TTS Model (XTTS-v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stage_header(\"Loading TTS Model\", 3, 3)\n",
    "\n",
    "print(\"Loading XTTS-v2...\")\n",
    "start_time = time.time()\n",
    "\n",
    "tts_model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "tts_memory = get_gpu_memory()\n",
    "\n",
    "print(f\"\\n✓ TTS loaded in {load_time:.2f}s\")\n",
    "print(f\"Total GPU Memory: {tts_memory['allocated_gb']:.3f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"  ALL MODELS LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal GPU Memory Usage: {tts_memory['allocated_gb']:.3f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path, language=\"spanish\"):\n",
    "    \"\"\"Step 1: ASR - Audio to Text\"\"\"\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = asr_pipe(\n",
    "        audio_path,\n",
    "        generate_kwargs={\"language\": language, \"task\": \"transcribe\"}\n",
    "    )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    memory = get_gpu_memory()\n",
    "    \n",
    "    return {\n",
    "        \"text\": result[\"text\"],\n",
    "        \"time\": inference_time,\n",
    "        \"memory\": memory[\"peak_gb\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_response(text, system_prompt=None, max_tokens=200):\n",
    "    \"\"\"Step 2: LLM - Text to Response\"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = \"Eres un asistente de voz útil y amigable. Responde de manera concisa, clara y en español.\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": text}\n",
    "    ]\n",
    "    \n",
    "    chat_text = llm_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = llm_tokenizer(chat_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = inputs.to(llm_model.device)\n",
    "    \n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=llm_tokenizer.pad_token_id,\n",
    "            eos_token_id=llm_tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = llm_tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    memory = get_gpu_memory()\n",
    "    num_tokens = len(generated_ids)\n",
    "    \n",
    "    return {\n",
    "        \"text\": response,\n",
    "        \"time\": inference_time,\n",
    "        \"tokens\": num_tokens,\n",
    "        \"tokens_per_sec\": num_tokens / inference_time if inference_time > 0 else 0,\n",
    "        \"memory\": memory[\"peak_gb\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def synthesize_speech(text, output_path, speaker_wav=None, language=\"es\"):\n",
    "    \"\"\"Step 3: TTS - Text to Audio\"\"\"\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tts_model.tts_to_file(\n",
    "        text=text,\n",
    "        file_path=output_path,\n",
    "        speaker_wav=speaker_wav,\n",
    "        language=language\n",
    "    )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # Get audio duration\n",
    "    audio_data, sample_rate = sf.read(output_path)\n",
    "    audio_duration = len(audio_data) / sample_rate\n",
    "    \n",
    "    memory = get_gpu_memory()\n",
    "    \n",
    "    return {\n",
    "        \"output_path\": output_path,\n",
    "        \"time\": inference_time,\n",
    "        \"audio_duration\": audio_duration,\n",
    "        \"rtf\": inference_time / audio_duration if audio_duration > 0 else 0,\n",
    "        \"memory\": memory[\"peak_gb\"]\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Pipeline functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Full Pipeline Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_pipeline(audio_input, speaker_reference=None, test_name=\"Test\"):\n",
    "    \"\"\"\n",
    "    Run complete voice assistant pipeline and measure performance.\n",
    "    \n",
    "    Args:\n",
    "        audio_input: Path to input audio file\n",
    "        speaker_reference: Path to speaker reference for TTS (optional)\n",
    "        test_name: Name for this test\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with complete results and timings\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"  RUNNING FULL PIPELINE: {test_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    pipeline_start = time.time()\n",
    "    results = {\"test_name\": test_name}\n",
    "    \n",
    "    # Stage 1: ASR\n",
    "    print(\"\\n[1/3] 🎤 ASR: Transcribing audio...\")\n",
    "    asr_result = transcribe_audio(audio_input)\n",
    "    results[\"transcription\"] = asr_result[\"text\"]\n",
    "    results[\"asr_time\"] = asr_result[\"time\"]\n",
    "    results[\"asr_memory\"] = asr_result[\"memory\"]\n",
    "    print(f\"      Text: {asr_result['text']}\")\n",
    "    print(f\"      Time: {asr_result['time']:.2f}s\")\n",
    "    \n",
    "    # Stage 2: LLM\n",
    "    print(\"\\n[2/3] 🤖 LLM: Generating response...\")\n",
    "    llm_result = generate_response(asr_result[\"text\"])\n",
    "    results[\"response\"] = llm_result[\"text\"]\n",
    "    results[\"llm_time\"] = llm_result[\"time\"]\n",
    "    results[\"llm_tokens\"] = llm_result[\"tokens\"]\n",
    "    results[\"llm_tokens_per_sec\"] = llm_result[\"tokens_per_sec\"]\n",
    "    results[\"llm_memory\"] = llm_result[\"memory\"]\n",
    "    print(f\"      Response: {llm_result['text']}\")\n",
    "    print(f\"      Time: {llm_result['time']:.2f}s ({llm_result['tokens_per_sec']:.1f} tokens/s)\")\n",
    "    \n",
    "    # Stage 3: TTS\n",
    "    print(\"\\n[3/3] 🔊 TTS: Synthesizing speech...\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = f\"pipeline_output_{timestamp}.wav\"\n",
    "    tts_result = synthesize_speech(llm_result[\"text\"], output_path, speaker_reference)\n",
    "    results[\"output_audio\"] = output_path\n",
    "    results[\"tts_time\"] = tts_result[\"time\"]\n",
    "    results[\"tts_audio_duration\"] = tts_result[\"audio_duration\"]\n",
    "    results[\"tts_rtf\"] = tts_result[\"rtf\"]\n",
    "    results[\"tts_memory\"] = tts_result[\"memory\"]\n",
    "    print(f\"      Output: {output_path}\")\n",
    "    print(f\"      Time: {tts_result['time']:.2f}s (RTF: {tts_result['rtf']:.2f}x)\")\n",
    "    \n",
    "    # Calculate totals\n",
    "    pipeline_end = time.time()\n",
    "    results[\"total_time\"] = pipeline_end - pipeline_start\n",
    "    results[\"processing_time\"] = asr_result[\"time\"] + llm_result[\"time\"] + tts_result[\"time\"]\n",
    "    results[\"overhead_time\"] = results[\"total_time\"] - results[\"processing_time\"]\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  PIPELINE SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\n⏱️  Timings:\")\n",
    "    print(f\"   ASR:         {results['asr_time']:6.2f}s ({results['asr_time']/results['total_time']*100:5.1f}%)\")\n",
    "    print(f\"   LLM:         {results['llm_time']:6.2f}s ({results['llm_time']/results['total_time']*100:5.1f}%)\")\n",
    "    print(f\"   TTS:         {results['tts_time']:6.2f}s ({results['tts_time']/results['total_time']*100:5.1f}%)\")\n",
    "    print(f\"   Overhead:    {results['overhead_time']:6.2f}s ({results['overhead_time']/results['total_time']*100:5.1f}%)\")\n",
    "    print(f\"   {'─'*30}\")\n",
    "    print(f\"   TOTAL:       {results['total_time']:6.2f}s\")\n",
    "    \n",
    "    print(f\"\\n💾 Peak Memory:\")\n",
    "    print(f\"   ASR:         {results['asr_memory']:.3f} GB\")\n",
    "    print(f\"   LLM:         {results['llm_memory']:.3f} GB\")\n",
    "    print(f\"   TTS:         {results['tts_memory']:.3f} GB\")\n",
    "    print(f\"   Max:         {max(results['asr_memory'], results['llm_memory'], results['tts_memory']):.3f} GB\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Pipeline with Audio Input\n",
    "\n",
    "You can either:\n",
    "1. Record audio now\n",
    "2. Use an existing audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Record new audio\n",
    "# Uncomment the line below to record\n",
    "# test_audio = record_audio(duration=5)\n",
    "\n",
    "# Option 2: Use existing audio file\n",
    "test_audio = \"test_input.wav\"  # Replace with your audio file\n",
    "\n",
    "# Set speaker reference for TTS (optional)\n",
    "speaker_reference = None  # Replace with your speaker reference if available\n",
    "# speaker_reference = \"my_voice_reference.wav\"\n",
    "\n",
    "print(f\"\\nUsing audio file: {test_audio}\")\n",
    "if speaker_reference:\n",
    "    print(f\"Using speaker reference: {speaker_reference}\")\n",
    "else:\n",
    "    print(\"⚠️  No speaker reference - TTS may require one. Add path to speaker_reference variable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Single Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline\n",
    "if Path(test_audio).exists():\n",
    "    result = run_full_pipeline(\n",
    "        audio_input=test_audio,\n",
    "        speaker_reference=speaker_reference,\n",
    "        test_name=\"Single Test\"\n",
    "    )\n",
    "    \n",
    "    # Play the output\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"🔊 Playing output audio...\")\n",
    "    print(\"=\"*70)\n",
    "    display(Audio(result[\"output_audio\"], autoplay=False))\n",
    "else:\n",
    "    print(f\"\\n⚠️  Audio file not found: {test_audio}\")\n",
    "    print(\"Please record audio or provide a valid file path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Multiple Pipeline Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple tests to get average performance\n",
    "num_tests = 3\n",
    "all_results = []\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  RUNNING {num_tests} PIPELINE TESTS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for i in range(num_tests):\n",
    "    print(f\"\\n\\n{'#'*70}\")\n",
    "    print(f\"#  TEST {i+1}/{num_tests}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    # Record audio for each test\n",
    "    test_audio = record_audio(duration=5)\n",
    "    \n",
    "    # Run pipeline\n",
    "    result = run_full_pipeline(\n",
    "        audio_input=test_audio,\n",
    "        speaker_reference=speaker_reference,\n",
    "        test_name=f\"Test {i+1}\"\n",
    "    )\n",
    "    \n",
    "    all_results.append(result)\n",
    "    \n",
    "    # Brief pause between tests\n",
    "    if i < num_tests - 1:\n",
    "        print(\"\\n⏸️  Pausing 3 seconds before next test...\")\n",
    "        time.sleep(3)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*70)\n",
    "print(\"  ALL TESTS COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n📊 Average Timings:\")\n",
    "    print(f\"   ASR:         {df['asr_time'].mean():6.2f}s  (±{df['asr_time'].std():.2f}s)\")\n",
    "    print(f\"   LLM:         {df['llm_time'].mean():6.2f}s  (±{df['llm_time'].std():.2f}s)\")\n",
    "    print(f\"   TTS:         {df['tts_time'].mean():6.2f}s  (±{df['tts_time'].std():.2f}s)\")\n",
    "    print(f\"   Overhead:    {df['overhead_time'].mean():6.2f}s  (±{df['overhead_time'].std():.2f}s)\")\n",
    "    print(f\"   {'─'*30}\")\n",
    "    print(f\"   TOTAL:       {df['total_time'].mean():6.2f}s  (±{df['total_time'].std():.2f}s)\")\n",
    "    \n",
    "    print(\"\\n💾 Memory Usage:\")\n",
    "    print(f\"   ASR Peak:    {df['asr_memory'].mean():.3f} GB\")\n",
    "    print(f\"   LLM Peak:    {df['llm_memory'].mean():.3f} GB\")\n",
    "    print(f\"   TTS Peak:    {df['tts_memory'].mean():.3f} GB\")\n",
    "    print(f\"   Max Peak:    {df[['asr_memory', 'llm_memory', 'tts_memory']].max().max():.3f} GB\")\n",
    "    \n",
    "    print(\"\\n🚀 Performance Metrics:\")\n",
    "    print(f\"   LLM Speed:   {df['llm_tokens_per_sec'].mean():.1f} tokens/s\")\n",
    "    print(f\"   TTS RTF:     {df['tts_rtf'].mean():.2f}x (lower is better)\")\n",
    "    \n",
    "    print(\"\\n📈 Time Distribution:\")\n",
    "    total_proc_time = df[['asr_time', 'llm_time', 'tts_time']].sum(axis=1).mean()\n",
    "    print(f\"   ASR:         {(df['asr_time'].mean() / total_proc_time * 100):.1f}%\")\n",
    "    print(f\"   LLM:         {(df['llm_time'].mean() / total_proc_time * 100):.1f}%\")\n",
    "    print(f\"   TTS:         {(df['tts_time'].mean() / total_proc_time * 100):.1f}%\")\n",
    "    \n",
    "    # Detailed results table\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Detailed Results:\")\n",
    "    print(\"=\"*70)\n",
    "    display_df = df[['test_name', 'asr_time', 'llm_time', 'tts_time', 'total_time']].copy()\n",
    "    display_df.columns = ['Test', 'ASR (s)', 'LLM (s)', 'TTS (s)', 'Total (s)']\n",
    "    print(display_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\n⚠️  No results to analyze. Run tests first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_results:\n",
    "    avg_total = df['total_time'].mean()\n",
    "    max_memory = df[['asr_memory', 'llm_memory', 'tts_memory']].max().max()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"  CONCLUSIONS & RECOMMENDATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n✅ Configuration Tested:\")\n",
    "    print(\"   - ASR: Whisper Small (244M params)\")\n",
    "    print(\"   - LLM: Qwen2.5-7B-Instruct (4-bit quantization)\")\n",
    "    print(\"   - TTS: XTTS-v2\")\n",
    "    \n",
    "    print(f\"\\n📊 Results:\")\n",
    "    print(f\"   - Average Response Time: {avg_total:.1f}s\")\n",
    "    print(f\"   - Peak GPU Memory: {max_memory:.1f} GB\")\n",
    "    \n",
    "    print(\"\\n💡 Real-time Performance:\")\n",
    "    if avg_total < 10:\n",
    "        print(\"   ✅ EXCELLENT - Suitable for interactive voice assistant\")\n",
    "    elif avg_total < 15:\n",
    "        print(\"   ✅ GOOD - Acceptable for voice assistant use\")\n",
    "    else:\n",
    "        print(\"   ⚠️  SLOW - May need optimization for real-time use\")\n",
    "    \n",
    "    print(\"\\n💾 Memory Efficiency:\")\n",
    "    if max_memory < 10:\n",
    "        print(\"   ✅ EXCELLENT - Fits comfortably within 12GB VRAM\")\n",
    "    elif max_memory < 12:\n",
    "        print(\"   ✅ GOOD - Within 12GB VRAM limit\")\n",
    "    else:\n",
    "        print(\"   ⚠️  OVER LIMIT - Exceeds 12GB VRAM constraint\")\n",
    "    \n",
    "    print(\"\\n🎯 Bottleneck Analysis:\")\n",
    "    slowest_stage = df[['asr_time', 'llm_time', 'tts_time']].mean().idxmax()\n",
    "    stage_names = {'asr_time': 'ASR', 'llm_time': 'LLM', 'tts_time': 'TTS'}\n",
    "    print(f\"   Slowest Stage: {stage_names[slowest_stage]}\")\n",
    "    \n",
    "    if slowest_stage == 'llm_time':\n",
    "        print(\"   Recommendation: LLM is the bottleneck\")\n",
    "        print(\"   - Reduce max_new_tokens for faster responses\")\n",
    "        print(\"   - Consider using smaller model or better quantization\")\n",
    "    elif slowest_stage == 'asr_time':\n",
    "        print(\"   Recommendation: ASR is the bottleneck\")\n",
    "        print(\"   - Consider using Distil-Whisper for faster transcription\")\n",
    "    else:\n",
    "        print(\"   Recommendation: TTS is the bottleneck\")\n",
    "        print(\"   - Consider caching common responses\")\n",
    "        print(\"   - Optimize audio chunk processing\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✅ This configuration is suitable for a voice assistant!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del asr_model, asr_processor, asr_pipe\n",
    "del llm_model, llm_tokenizer\n",
    "del tts_model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"\\n✓ Cleanup complete\")\n",
    "    print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested the complete voice assistant pipeline:\n",
    "\n",
    "**Pipeline:** Audio → ASR → LLM → TTS → Audio\n",
    "\n",
    "**Key Metrics:**\n",
    "- End-to-end latency\n",
    "- Individual service timings\n",
    "- GPU memory usage\n",
    "- Performance bottlenecks\n",
    "\n",
    "**Configuration:**\n",
    "- ASR: Whisper Small (~2GB VRAM)\n",
    "- LLM: Qwen2.5-7B 4-bit (~5GB VRAM)\n",
    "- TTS: XTTS-v2 (~2GB VRAM)\n",
    "- **Total: ~9-10GB VRAM** ✅\n",
    "\n",
    "This setup provides a good balance of quality, speed, and memory efficiency for a voice assistant application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
